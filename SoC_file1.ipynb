{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Function to generate sine wave tasks\n",
    "def generate_sine_wave(amplitude, phase, num_samples):\n",
    "    x = np.linspace(-5, 5, num_samples)\n",
    "    y = amplitude * np.sin(x + phase)\n",
    "    return torch.tensor(x, dtype=torch.float32).unsqueeze(1), torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# MAML training loop\n",
    "def maml_training(model, tasks, meta_lr, inner_lr, num_inner_steps, meta_batch_size, num_iterations):\n",
    "    meta_optimizer = optim.Adam(model.parameters(), lr=meta_lr)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        meta_optimizer.zero_grad()\n",
    "\n",
    "        meta_loss = 0\n",
    "        for _ in range(meta_batch_size):\n",
    "            # Sample a task\n",
    "            amplitude, phase = np.random.uniform(0.1, 5.0), np.random.uniform(0, np.pi)\n",
    "            x_train, y_train = generate_sine_wave(amplitude, phase, 10)\n",
    "            x_val, y_val = generate_sine_wave(amplitude, phase, 10)\n",
    "            \n",
    "            # Inner loop: Clone the model and perform gradient descent\n",
    "            cloned_model = SimpleNN(1, 40, 1)\n",
    "            cloned_model.load_state_dict(model.state_dict())\n",
    "            inner_optimizer = optim.SGD(cloned_model.parameters(), lr=inner_lr)\n",
    "\n",
    "            for _ in range(num_inner_steps):\n",
    "                y_pred = cloned_model(x_train)\n",
    "                loss = nn.MSELoss()(y_pred, y_train)\n",
    "                inner_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                inner_optimizer.step()\n",
    "\n",
    "            # Outer loop: Compute validation loss\n",
    "            y_val_pred = cloned_model(x_val)\n",
    "            val_loss = nn.MSELoss()(y_val_pred, y_val)\n",
    "            meta_loss += val_loss\n",
    "        \n",
    "        # Meta optimization step\n",
    "        meta_loss /= meta_batch_size\n",
    "        meta_loss.backward()\n",
    "        meta_optimizer.step()\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Meta Loss: {meta_loss.item()}\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 1\n",
    "hidden_dim = 40\n",
    "output_dim = 1\n",
    "meta_lr = 0.001\n",
    "inner_lr = 0.01\n",
    "num_inner_steps = 1\n",
    "meta_batch_size = 5\n",
    "num_iterations = 1000\n",
    "\n",
    "# Initialize model and train\n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "maml_training(model, generate_sine_wave, meta_lr, inner_lr, num_inner_steps, meta_batch_size, num_iterations)\n",
    "\n",
    "# Testing the trained model on a new task\n",
    "amplitude, phase = 2.5, 0.5\n",
    "x_test, y_test = generate_sine_wave(amplitude, phase, 50)\n",
    "y_pred = model(x_test).detach().numpy()\n",
    "plt.plot(x_test, y_test, label='True')\n",
    "plt.plot(x_test, y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
